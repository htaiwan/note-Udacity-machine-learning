# 深度神經網路

## Agenda

- [連續感知器](#1)
- [非線性模型](#2)
- [神經網路結構](#3)
- [前向回饋](#4)
- [反向傳播](#5)
- [Keras](#6)
- [Keras Lab - 錄取學生](#7)
- [訓練優化](#8)
- [早期停止](#9)
- [正則化](#10)
- [Dropout](#11)
- [局部最低點](#12)
- [隨機重新開始](#17)
- [動量](#18)
- [梯度消失](#13)
- [其他激活函數](#14)
- [批次與隨機梯度下降](#15)
- [學習速率衰退](#16)
- [Keras中的優化程序](#19)
- [已知的誤差函數](#20)
- [神經網路回歸](#21)
- [嘗試神經網路](#22)
- [Keras Lab - IMDB數據](#23)

## Note

<h2 id="1">連續感知器</h2>

- 簡短的回憶之前學到感知器。
	- 藍色區域點是藍色的機率較大。
	- 紅色區域點是紅色的機率比較大。
- 將weight跟feature抽離變成感知器模型。

![235](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/235.png)

<h2 id="2">非線性模型</h2>

- 非線性數據: 無法用單純的直線分割資料。

![236](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/236.png)

- 對於這樣的非線性數據，**如何透過神經網路構造出一個機率函數**。
	- 讓藍色區域點是藍色的機率較大。
	- 讓紅色區域點是紅色的機率比較大。

![237](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/237.png)

<h2 id="3">神經網路結構</h2>

#### 基本架構

- **如何透過神經網路構造出一個機率函數**
	- 對現有的線性模型再進行一次線性組合，得到複雜的新模型。
		
![238](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/238.png)

- 1. 將兩個線性模型轉化成感知器架構。
	
![239](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/239.png)

- 2. 對現有的線性模型再進行一次線性組合。

![240](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/240.png)

![241](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/241.png)

- 3. 另外一種相同表示方式，將bias抽離。

![242](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/242.png)

#### 多層級

- 基本三層:input, hidden, output。

![243](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/243.png)

- 變化型1: hidden layer中的模型增加。

![244](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/244.png)

- 變化型3: input layer中的feature增加。

![245](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/245.png)

- 變化型4: out layer中的output增加。

![246](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/246.png)

- 變化型5: hidden layer中的layer增加。

![247](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/247.png)

- **深度神經網路完整結構**。

![248](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/248.png)

#### 多類別分類

- 透過softmax函數來進行多類別分類。

![249](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/249.png)

<h2 id="4">前向回饋</h2>

- **Feedfoward: 神經網路用來將輸入變成輸出的流程。**

![250](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/250.png)

- 單層hidden layer的數學表達。

![251](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/251.png)

- 多層hidden layer的數學表達。

![252](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/252.png)

#### 誤差函數

- 神經網路仍會產生誤差函數，最終我們必須透過不斷的降低誤差來優化神經網路。

- 單層hidden layer的誤差函數。
	- 跟之前學的linear regression中的誤差函數是一樣。
	- 因為神經網路的每一層的運作，就是將上一層的輸出**進行線性組合**在輸出。
	- 因為都是進行線性組合，所以誤差函數就是linear regression中的誤差函數表達一樣。

![253](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/253.png)

- 多層hidden layer的誤差函數。
	- 不管層數再多，誤差函數仍是相同的定義(因為每一層都是進行線性組合)，唯一的變化是誤差函數中y^越來越複雜。

![254](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/254.png)


<h2 id="5">反向傳播</h2>

- **Backpropgation: 用來訓練神經網路的一個流程。**
	- 先進行feedward。
	- 將模型的輸出與期望的輸出做比較。
	- 計算誤差。
	- 向後進行反向傳播，將誤差分散到每個權重上。
	- 更新權重，並獲得更好的模型。
	- 反覆持續這樣的流程，直到獲得我們需求的模型。

- 例子: 由下方的圖觀察，最終模型是誤判了(x1,x2)這個點，我們期望他是藍點，但模型預測他在紅色邊界內。觀察前一層的輸入，發現前一層下方的模型是判斷正確，所以要做的就是**調大下方正確模型的權重，調降上方錯誤模型的權重。**

![255](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/255.png)

#### 反向傳播數學

- 先回顧最簡單的模型，沒有任何的hidden layer，也就是linear regression。

![257](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/257.png)

- 多層hidden layer。
	- y^的產生方式變複雜。
	- Error function仍不變。
	- gradient的目標就是把error分散到圖形中每條邊上。

![258](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/258.png)

- 一層hidden layer為例子。
	- 目標就是計算出所有的W對E的微分。

![259](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/259.png)


#### 鏈式法則

- 對複合函數求導數，就是一系列導數的乘積。
	- 神經網路的前向回饋就是進行一系列的複合函數的運算，因此鏈式法則剛好用來幫助我們處理反向傳播。

![260](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/260.png)

#### 反向傳播例子說明

- 前向回饋就是進行一系列的複合函數的運算。

![261](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/261.png)

- 如何透過鏈式法則，計算出每條邊應該得到的誤差。

![262](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/262.png)

- 放大細看最後一層。

![263](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/263.png)

- [Backpropagation 算法的推导与直观图解](https://www.cnblogs.com/andywenzhi/p/7295262.html)
- [Backpropagation Algorithm](https://www.coursera.org/lecture/machine-learning/backpropagation-algorithm-1z9WW)
- [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
- [simple-neural-network](https://github.com/mattm/simple-neural-network)

<h2 id="6">Keras</h2>

#### 用Keras 构建神经网络

- 序列模型:
	- keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。

```python
from keras.models import Sequential

#Create the Sequential model
model = Sequential()
```

- 层:
	- Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 add() 函数添加层。
	- 上面的第一层 model.add(Dense(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。

```python
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten

#创建序列模型
model = Sequential()

#第一层 - 添加有128个节点的全连接层以及32个节点的输入层
model.add(Dense(128, input_dim=32))

#第二层 - 添加 softmax 激活层
model.add(Activation('softmax'))

#第三层 - 添加全连接层
model.add(Dense(10))

#第四层 - 添加 Sigmoid 激活层
model.add(Activation('sigmoid'))
```

- 构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，稍后我们将了解这一概念，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。

```python
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])
```

- 我们可以使用以下命令来查看模型架构：

```python
model.summary()
```

- 使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。然后使用fit命令训练模型并通过 epoch 参数来指定训练轮数（周期），每 epoch 完成对整数据集的一次遍历。 verbose 参数可以指定显示训练过程信息类型，这里定义为 0 表示不显示信息。

- 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。

```python
model.fit(X, y, nb_epoch=1000, verbose=0)
```

- 最后，我们可以使用以下命令来评估模型：

```python
model.evaluate()
```

<h2 id="7">Keras Lab - 錄取學生</h2>

- [Keras Lab - 錄取學生](https://github.com/htaiwan/note-Udacity-machine-learning/tree/master/Jupyter/StudentAdmissionsKeras-zh.ipynb)

<h2 id="8">訓練優化</h2>

- 在訓練過程常常會發現結果並未預期的好，原因?
	- 所選的結構可能不合適。
	- 數據太多瑕疵。
- 另外一種主要原因，數據訓練可能需要很久的時間，但我們希望可以運行更快，所以要進行優化，可以讓模型在最短的時間達到最佳的效果。

<h2 id="9">早期停止</h2>

- 觀察不同的epoh的training跟test error的變化。

![264](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/264.png)

- **Model Complexity Graph**
	- 將上面觀察到變化，轉成underfitting, just right跟overfitting的概念。
	- **Early Stop: 就是在測試誤差停止降低，並開始變大時，我們就停止訓練**

![265](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/265.png)

<h2 id="10">正則化</h2>

- 問題: 兩個模型的prediction function只有差在係數大小不同，那麼哪個模型是產生較小的誤差？
- 答案: 因為sigmoid的因素，係數大的模型產生的誤差較小，但真的這樣模型就是較佳的模型嗎？
- **這樣的模型並不是好的模型，因為overfitting了**

![267](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/267.png)

- 從數學角度來看:
	- 係數小(左邊)
		- 在執行梯度下降時，有較好的斜率
	- 係數大(右邊)
		- 梯度下降會變得沒什麼效用，雖然sigmoid的函數更快靠近0,1，相對導數也接近0，在梯度變化區域非常陡峭的地方，導數會變得非常大。
	- 因為右邊的模型太過於穩定，很難使用梯度下降法，所以為了可以合理使用梯度下降法，我們會偏向使用左邊的模型。

![266](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/266.png)

- 正則化: 就是要避免上面的問題，因為太大的係數容易遇到overfitting。
	- 但由於較大的係數可以產生較小的誤差，所以要再error function上做些修正，而這個修正就是正則化。

![268](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/268.png)

- 正則化的核心思想:
	- 越複雜的係數，可以降低原本error function的誤差，但相對會提升L1, L2的值，因此機器在進行學習時，機器會想辦法再原本error function跟L1,L2的值之間取個平衡。


- L1,L2的正則化差異: [這邊的解釋，比課堂得清楚](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-09-l1l2regularization/)
	
	- L1: 使用 L1 的方法, 我们很可能得到的结果是只有 theta1 的特征被保留，所以很多人也用L1正规化来挑选对结果贡献最大的重要特征. 但是 L1 的结并不是稳定的. 比如用批数据训练, 每次批数据都会有稍稍不同的误差曲线。
	- L2: 这种变动, 白点的移动不会太大, 而 L1的白点则可能跳到许多不同的地方 , 因为这些地方的总误差都是差不多的. 侧面说明了 L1 解的不稳定性。

	![270](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/270.png)
	
	![271](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/271.png)

- 後來想想，似乎課堂上的說明也不差 ！！
	- L1: 因為是權重的絕對值相加。所以當有兩組不同的參數組(w1, w2) = (1, 0), (w1, w2) = (1, -1)，L1會選擇(1, 0)因為產生的絕對值和較小。
		- 所以L1會挑選出比較重要特徵，給他較大的權重。
	- L2: 因為是權重的平方相加。所以當有兩組不同的參數組(w1, w2) = (1, 0), (w1, w2) = (0.5, 0.5)，L2會選擇(0.5, 0.5)因為產生的平方和較小。
		- 所以L2是盡可能平均優化每個權重，產生的模型也會比較穩定。

![269](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/269.png)

<h2 id="11">Dropout</h2>

- 有些網路的某個部份，權重非常大，最終對訓練起了主要作用，但有些部分，並沒有起到多大作用，所以都沒被訓練到。為了避免這樣狀況發生，於是有了**dropout**。

![272](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/272.png)

- **dropout**: 在固定次數的epoch後，就隨機移除某些node後，再繼續進行訓練，讓每個node都要機會被平均的訓練到。

![273](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/273.png)

<h2 id="12">局部最低點</h2>

- Gradient descent的盲點。無法透過Gradient descent來更近一步優化，必須透過其他方式。
- **要如何解決這個問題??**

![274](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/274.png)

<h2 id="17">隨機重新開始</h2>

- 如何避免掉到局部最低點？那就是從幾個隨機的不同地點開始，對所有這些地點進行gradient descent，增大達到全局最低點的機率。

![285](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/285.png)

<h2 id="18">動量</h2>

- 另外一個解決局部最低點的方法。
- **Momentum**
	- 每次的步長都會參考之前的步長才來決定。
	- 距離越久的步長，影響越小。
- 這樣的方式，可以幫助我們有機會翻過駝峰，增大達到全局最低點的機率。

![286](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/286.png)

<h2 id="13">梯度消失</h2>

- 觀察sigmoid的兩端導數幾乎趨近於0，這樣會影響到Gradient descent效率，因為Gradient descent就是透過導數來告訴我們移動的方向。

![275](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/275.png)

- 在深度神經網路中，更容易遇到這樣梯度消失的狀況。
	- ex. 要求算是左邊的值，根據連鎖率，會轉換成右邊的一堆導數的乘積，而每一個導數又都是sigmoid函數，所以他們都很小，一堆很小的數字相乘，又變得更小。
	- 意味著每次邁出的步伐都很小，如此一來就很難達到最佳值。
- **要如何解決這個問題??**

![276](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/276.png)

<h2 id="14">其他激活函數</h2>

- **透過其他激活函數來取代sigmoid，以避免深度神經網路中梯度消失的情況。**

- tanh: -1到1之間的導數比sigmoid大。

![277](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/277.png)

- ReLU: 如果值是正的，導數為1。

![278](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/278.png)

- 利用其他更好的激活函數，讓求導數乘積會變成更大的數字。

![279](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/279.png)

<h2 id="15">批次與隨機梯度下降</h2>

- epoch: 每次gradient descent跑完一次，更新所有權重一次，這就稱做一次epoch。

![280](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/280.png)

- Batch: 拿所有資料去跑gradient descent，再來更新所有權重。
	- 缺點：每一次epoch的計算都相當費時。

![281](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/281.png)

- Stochastic: 將資料隨機分組，每次拿一組資料去跑gradient descent。
	- 現實中，採取大量稍微不太準確的步長會比採取一步很準確的步長好很多。

![282](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/282.png)

<h2 id="16">學習速率衰退</h2>

- 學習速率太大會錯過最佳值。
- 學習速率太小會很久才會達到最佳值。

![283](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/283.png)

- 在陡峭的地方，學習速率就設大一點。
- 在平緩的地方，學習速率就設小一點。

![284](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/284.png)

<h2 id="19">Keras中的優化程序</h2>

- Keras 中有很多优化程序，建议你访问此[链接](https://keras.io/optimizers/)或这篇精彩[博文](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)，详细了解这些优化程序。
- SGD: 这是随机梯度下降。它使用了以下参数：
	- 学习速率。
	- 动量（获取前几步的加权平均值，以便获得动量而不至于陷在局部最低点）。
	- Nesterov 动量（当最接近解决方案时，它会减缓梯度）
- Adam: (Adaptive Moment Estimation) 
	- 使用更复杂的指数衰减，不仅仅会考虑平均值（第一个动量），并且会考虑前几步的方差（第二个动量）。
- RMSProp:(RMS 表示均方根误差）通过除以按指数衰减的平方梯度均值来减小学习速率。 

<h2 id="20">已知的誤差函數</h2>
 - 不知道這節重點是啥？？怪 ？？

<h2 id="21">神經網路回歸</h2>

- 是否可以將神經網路運用到回歸問題? 

![287](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/287.png)

- 目前學到神經網路架構，最後透過sigmoid得到0~1的值，來處理**分類問題的機率大小。**

![288](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/288.png)

- 移除最後sigmoid拿到原本的值，就樣就可以處理**回歸問題。**

![289](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/289.png)

- 分別拿Relu跟sigmoid當作activation funciton，所得到得線性組合，可以用來處理不同回歸問題。

![290](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/290.png)

![291](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/291.png)

<h2 id="22">嘗試神經網路</h2>

- [神经网络基础知识的视觉和互动指南](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)

<h2 id="23">Keras Lab - IMDB數據</h2>

- [Keras Lab - IMDB數據](https://github.com/htaiwan/note-Udacity-machine-learning/tree/master/Jupyter/IMDB_In_Keras-zh.ipynb)




