# 強化學習框架-解決方法

## Agenda

- [策略](#1)
- [網格世界](#2)
- [狀態值函數](#3)
- [貝爾曼方程式-part 1](#4)
- [最優性](#5)
- [動作值函數](#6)
- [最優策略](#7)
- [貝爾曼方程式-part 2](#5)
- [總結](#6)

## Note

<h2 id="1">策略</h2>

- 智能體如何根據任何環境的狀態作出合適的動作響應。
- 策略(Policy): 一個mapping方式，將state映射到action。

![507](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/507.png)

- 決定性策略: 輸入一個state只會對應到某個action。

![508](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/508.png)

- 隨機性策略: 輸入一組(state, action)，輸出機率(智能體會根據此state做此action的機率)。

![509](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/509.png)

- 例子

![510](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/510.png)

- 決定性策略都可以轉化成隨機性策略

![511](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/511.png)

<h2 id="2">網格世界</h2>

- 將透過網格世界這個例子來講解如何定義最佳策略。
	- 環境是個九宮格的世界, 其中有兩格是山。
	- 智能體從左上出發，目標是最短時間內達到右下角的出口。
	- 每走一步reward = -1。
	- 若碰到山reward = -3。
	- 達到終點reward = 5。

![512](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/512.png)


<h2 id="3">狀態值函數</h2>

- 先從一個最糟糕的策略開始，就是智能體經過九宮格的每一個。
	- 產生裝狀態值 -6

![513](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/513.png)

- 接下來智能體向右一格，當作出發點，仍採用最糟糕的策略，需經過每一個。
   - 產生裝狀態值 -5

![514](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/514.png)

- 依此，完成九宮格所有的狀態值。

![515](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/515.png)

- **狀態值函數**
  - 對於每個狀態s，它都告訴我們
  - 如果智能體是從該狀態s開始
  - 然後在所有的時間步都是根據此該策略選擇動作
  - 預期的折扣回報會是多少
  - 狀態值函數始終對應特定的策略
  - 如果我們更改策略，就會更改狀態值函數
  
![516](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/516.png)

<h2 id="4">貝爾曼方程式-part 1</h2>

- 上個例子，九宮格中的每一個狀態值的計算其實有很多重複的計算步驟。

![517](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/517.png)

- 減省計算的方式，是透過遞迴的方式，由尾倒推回去。
	- 任何時間的狀態值為
	- 當下的即時獎勵(reward=-1) + 下個時間的狀態值(2)。
	- 為了簡便，我們假設折扣率＝1。
	- 但通常都是要考慮到折扣率，也就是下個時間的狀態值要乘上折扣率才行。

![518](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/518.png)

- **貝爾曼方程式**: 將上面的概念轉化成方程式。

![519](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/519.png)

- 九宮格例子，我們做了下列簡化
   - 它始终沿着所选方向移动（而一般 MDP 则不同，智能体并非始终能够完全控制下个状态将是什么）
   - 可以确切地预测奖励（而一般 MDP 则不同，奖励是从概率分布中随机抽取的）
	- 對於更加複雜的環境，我們無法確定即時獎勵跟下個狀態。

![520](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/520.png)

<h2 id="5">最優性</h2>

- 兩種不同policy所產生的九宮格狀態值函數。
	- 右邊九宮格中任何一格狀態值皆大於左邊。

![521](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/521.png)

- 定義: 如果要說某個policy比較優秀，那就是在這個policy下所有時刻的狀態都優於另外一個policy。
	- 通常很容易找到滿足這樣狀況的policy。

- 定義: 肯定存在一個最優策略，比其他所有策略都好。
	- 最優策略並不一定是唯一。

![522](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/522.png)

<h2 id="6">動作值函數</h2>

-**動作值函數**:
	- 對於每一個狀態s跟動作a，都會生成一個動作值。
	- 智能體從狀態s開始並選擇動作獲得的期望這扣回報。
	- 然後使用該策略在未來所有的時間步動作。

![523](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/523.png)

- 注意事項

![524](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/524.png)

- 再次比較**狀態值函數**跟**動作值函數**。

![525](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/525.png)

- 例子1: **狀態**從左上角開始，**動作**採用向下移動，所產生的動作函數。

![526](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/526.png)

- 例子2: **狀態**從左邊第2格開始，**動作**採用向上移動，所產生的動作函數。

![527](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/527.png)

- 九宮格的最終所有動作函數值。

![528](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/528.png)

- 再次定義。

![529](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/529.png)


<h2 id="7">最優策略</h2>

- 要如何找出最優的策略。
	- 流程: 透過互動找出最佳的動作值函數，再從最佳的動作值函數找出最優的策略。
	- 這一節先假設我們已經找到最動作值函數。
	- 主要目的是學習如何從最佳的動作值函數找出最優的策略。


![530](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/530.png)

- step1:
	- 對於九宮格中的每一次，我們找出最佳的動作值，並決定對應的動作。

![531](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/531.png)

- step2:
	- 對於九宮格中左邊第二格，所有動作值都是相同，因此可以任意選擇一個方向，都會產生最優策略。

![531](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/531.png)

- 透過上述步驟，我們很快地從最佳的動作值函數找出最優的策略

![532](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/532.png)

<h2 id="8">貝爾曼方程式-part 2</h2>

- 推導流程:



<h2 id="9">總結</h2>

- 策略: 確定性跟隨機性兩種。

- 狀態值函數: 智能体从状态s开始，然后在所有时间步根据相同策略选择动作的预期回报。

- 貝爾曼方程-1:

- 最優性:
   - 什麼叫優(定義): 如果要說某個policy比較優秀，那就是在這個policy下所有時刻的狀態都優於另外一個policy。
   - 最优策略肯定存在，但并不一定是唯一的。
   - 所有最优策略都具有相同的状态值函数v*

- 動作值函數:
	- 所有最优策略具有相同的动作值函数q*，称之为最优动作值函数。

- 最優策略:
   - 智能體很快地從最佳的動作值函數找出最優的策略。

- 貝爾曼方程-2:



