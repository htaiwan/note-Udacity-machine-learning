# 蒙特卡洛方法

## Agenda

- [簡介](#1)
- [迷你項目 - 21點](#2)
- [MC預測: 狀態值](#3)
- [實現 - MC預測: 狀態值](#4)
- [迷你項目 part1](#5)
- [MC預測: 動作值](#6)
- [實現 - MC預測: 動作值](#7)
- [迷你項目 part2](#8)
- [廣義的策略迭代](#9)
- [MC控制: 增量均值](#10)
- [練習: 增量均值](#11)
- [MC控制: 策略評估](#12)
- [MC控制: 增量改進](#13)
- [練習: Epsilon 貪然策略](#14)
- [探索與利用](#15)
- [實現 - MC 控制：GLIE](#16)
- [迷你項目 part3](#17)
- [MC控制: 常量a part1](#18)
- [MC控制: 常量a part2](#19)
- [實現 - MC控制: 常量a](#20)
- [迷你項目 part4](#21)
- [總結](#22)


## Note

<h2 id="1">簡介</h2>

- 回顧之前所學的
	- 強化學習的基本框架(agent, environment, MDP)。
	- 動態規劃(簡化版的強化學習)。
		- 假設agent已經完全了解environment的動態特性。

- 另外一種強化學習問題，即是agent完全不了解environment的動態特性，必須透過互動來了解這個信息。


<h2 id="2">迷你項目 - 21點</h2>

- [github](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py)

> - "Simple blackjack environment
> 
> 
>   - Blackjack is a card game where the goal is to obtain cards that sum to as
    near as possible to 21 without going over.  They're playing against a fixed
    dealer.
    Face cards (Jack, Queen, King) have point value 10.
    Aces can either count as 11 or 1, and it's called 'usable' at 11.
    This game is placed with an infinite deck (or with replacement).
    The game starts with each (player and dealer) having one face up and one
    face down card.
    The player can request additional cards (hit=1) until they decide to stop
    (stick=0) or exceed 21 (bust).
    After the player sticks, the dealer reveals their facedown card, and draws
    until their sum is 17 or greater.  If the dealer goes bust the player wins.
    If neither player nor dealer busts, the outcome (win, lose, draw) is
    decided by whose sum is closer to 21.  The reward for winning is +1,
    drawing is 0, and losing is -1.
    The observation of a 3-tuple of: the players current sum,
    the dealer's one showing card (1-10 where 1 is ace),
    and whether or not the player holds a usable ace (0 or 1).
    
> - This environment corresponds to the version of the blackjack problem
    described in Example 5.1 in Reinforcement Learning: An Introduction
    by Sutton and Barto.
    http://incompleteideas.net/book/the-book-2nd.html


<h2 id="3">MC預測: 狀態值</h2>

- Episode: 在階段性任務中，智能體在時間T遇到最終裝態，互動結束的過程。
- 智能體的目標: 就是在這Episode找到一個最佳的策略，可以獲得最大的獎勵。

![578](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/578.png)

- Prediction Problem: 既然目標是找到一個最佳的策略，我們首先要先量化一個策略的優劣程度。
- On-Policy method: 透過觀察每個Episode中，智能體依據某個策略跟環境互動清況，來量化一個策略的優劣。

![579](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/579.png)

- 例子: 智能體依據某個策略跟環境互動產生3組Episode。

![580](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/580.png)

- 如何在量化策略X
	- 找到第一個X，計算後面所有reward和，在求平均。 

![581](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/581.png)

- 如何在量化策略Y(注意: Ｙ有重複出現多次)
	- visit: 再Episode中，每次出現Y，我們稱做visit。
	- first-visit MC method: 就跟上面量化x的方式一樣。
	- every-visit MC method: 計算每個visit在求平均。

![582](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/582.png)

![583](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/583.png)

![584](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/584.png)

<h2 id="4">實現 - MC預測: 狀態值</h2>

![585](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/585.png)

<h2 id="5">迷你項目 part1</h2>

```python
from collections import defaultdict
import numpy as np
import sys

def mc_prediction_v(env, num_episodes, generate_episode, gamma=1.0):
    # initialize empty dictionary of lists
    returns = defaultdict(list)
    # loop over episodes
    for i_episode in range(1, num_episodes+1):
        # monitor progress
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()
                
        # 產生一個與環境互動的episode
        # generate_episode外部所傳入的一個策略，根據此策略跟莊家完一回合(episode)
        episode = generate_episode(env)
        print("episode = ", episode)
        # 獲得所有的states, actions, and rewards
        states, actions, rewards = zip(*episode)
        print("rewards = ", rewards)
        # 準備折扣率
        discounts = np.array([gamma**i for i in range(len(rewards)+1)])
        print("discounts =", discounts)
        # 計算並記錄這個episode中每個visit
        for i, state in enumerate(states):
            returns[state].append(sum(rewards[i:]*discounts[:-(1+i)]))
            print("returns = ",returns)
              
    # 計算state-value function的推估值
    V = {k: np.mean(v) for k, v in returns.items()}

    return V
```

<h2 id="6">MC預測: 動作值</h2>

- 接下來，要從狀態值找出最佳的動作值，在之前動態規劃中，可以透過下列式子，讓我們從狀態值來決定出最佳的動作值。
	- 但是在強化學習中，並不知道這些與環境互動的動態特性，所以沒有辦法直接透過這個式子。

![586](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/586.png)

- 舉之前動態規劃中的相同例子

![587](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/587.png)

- 跟動態規劃中不同的是，不是單單只看狀態，而是看狀態跟緊接動作的配對。

![588](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/588.png)

- 定義Visit

![589](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/589.png)

- First Visit, Every Visit 
	- 雖然計算出兩種不同數字，但當agent執行的次數夠多時，這兩個數字會收斂成相同。

![590](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/590.png)

- 假設我們都是使用First Visit

![591](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/591.png)

- 因為例子中的策略是**決定性策略**，所以不管agent執行再多次，動作值函數估計結果始終不會是完整。

![592](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/592.png)

- 解決方案 - **採用隨機性策略**，在每個狀態，都有一定的機率經歷過每個動作。

![593](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/593.png)

- 一但agent執行次數夠多，就有機會獲得完美的動作值函數。

![594](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/594.png)

<h2 id="7">實現 - MC預測: 動作值</h2>

![595](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/595.png)

<h2 id="8">迷你項目 part2</h2>

```python
def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):
    # initialize empty dictionaries of arrays
    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))
    N = defaultdict(lambda: np.zeros(env.action_space.n))
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    # loop over episodes
    for i_episode in range(1, num_episodes+1):
        # monitor progress
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()
        
        # 根據所給定的策略(generate_episode)與環境進行互動，產生一個episode
        episode = generate_episode(env)
        # 獲得episode中每個states, actions跟rewards
        states, actions, rewards = zip(*episode)
        # 準備折扣率
        discounts = np.array([gamma**i for i in range(len(rewards)+1)])
        # loop每個state
        for i, state in enumerate(states):
            # 計算這個episode中每個state中agent決定某個action所獲得的return
            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])
            # 計算這個episode中每個state中agent決定執行這個action的次數
            N[state][actions[i]] += 1.0
            # 獲得episode中每個state中agent決定某個action的動作估算值
            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]
     
    return Q
```

<h2 id="9">廣義的策略迭代</h2>

- Control problem: Agent是如何與環境互動得出最佳策略。 

![596](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/596.png)

- 回顧之前動態規劃時的設置流程 - 各種不同迭代方式

![597](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/597.png)

- 廣義迭代 - 不對策略評估週期次數進行限制，也不對收斂程度進行限制。
 
![598](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/598.png)

<h2 id="10">MC控制: 增量均值</h2>

- 蒙特卡洛控制算法受到廣義迭代的啟發。
	- 在之前的練習中，Agent大概需要猜測5000次才能得到不錯的估算函數。
	- 這樣在進行策略改善之前，似乎發太多時間進行策略評估。

![599](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/599.png)

- 或許在每次猜測之後，都進行策略改善比較合理。

![600](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/600.png)

- 回顧一下，之前是如何得出估算函數的。

![601](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/601.png)

![602](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/602.png)

- **現在，我們不在所有階段結束後才計算平均值，而是在每次迭代之後都更新估計值。**

![603](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/603.png)

- 增量均值公式推導

![604](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/604.png)

- 增量均值算法
	
![605](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/605.png)

<h2 id="11">練習: 增量均值</h2>

```python

```

<h2 id="12">MC控制: 策略評估</h2>

- 回顧之前的增量均值
	- 如何利用新的return，直接更新估算值。

![606](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/606.png)

- 流程
	- 初始化每個對的經歷次數
	- Agent先從環境中得到一個階段的樣本。
	- 對於每個時間步，都要查看對應的(狀態-動作)對。
	- 如果是首次經歷，則計算對應的回報。
	- 然後根據增量均值算法，更新動作值的對應估值。

![607](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/607.png)

<h2 id="13">MC控制: 增量改進</h2>

- 回顧之前動態規劃時，是如何做增量改進的。
	- 在每個狀態下，我們選擇最高的動作(Greedy Policy)。

![608](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/608.png)

- Greedy Policy所遇到的問題。
	- 假設有兩道門，一開始是隨機開啟。
	- 在前面剛開始一段時間，不斷發現開啟門A都會獲得獎勵，但門B都沒有。
	- 此時Greedy Policy就會讓Agent不斷選擇A。
	- 但是門B其實有機會獲得更大的獎勵，只是機會較低，但這樣的Greedy Policy就會失去這樣的獎勵。

![609](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/609.png)

- **Epsilon-Greedy Policy**
	- 讓Greedy policy還是有一定的機率會選擇門B(非最高的動作值)。

![610](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/610.png)

![611](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/611.png)

- 整合到MC算法

![612](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/612.png)


<h2 id="14">練習: Epsilon 貪然策略</h2>

![614](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/614.png)

<h2 id="15">探索與利用</h2>

- 動態調整Epsilon。
	- 合理的做法是智能体一开始与环境互动时，倾向于探索环境，而不是利用已有的经验(Epsilon=1)。
	- 在后续时间步，合理的做法是倾向于利用已有的经验，而不是探索环境，策略在动作值函数估算方面越来越贪婪(Epsilon=0)。	

- 有限状态下的无限探索贪婪算法 (GLIE)
	- 在指定 ϵ 贪婪策略时，修改 ϵ 的值。


<h2 id="16">實現 - MC 控制：GLIE</h2>

![613](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/613.png)

<h2 id="17">迷你項目 part3</h2>

```python
# 根據epsilon利用epsilon-greedy policy產生episode
def generate_episode_from_Q(env, Q, epsilon, nA):
    episode = []
    state = env.reset()
    
    while True:
        # 利用epsilon-greedy policy來決定action
        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \
                                    if state in Q else env.action_space.sample()

        next_state, reward, done, info = env.step(action)
        episode.append((state, action, reward))
        state = next_state
        if done:
            break
            
    return episode

# epsilon-greedy policy
def get_probs(Q_s, epsilon, nA):
    policy_s = np.ones(nA) * epsilon / nA
    best_a = np.argmax(Q_s)
    policy_s[best_a] = 1 - epsilon + (epsilon / nA)
    return policy_s

# 利用episode來更新action-value function
def update_Q_GLIE(env, episode, Q, N, gamma):
    states, actions, rewards = zip(*episode)
    # prepare for discounting
    discounts = np.array([gamma**i for i in range(len(rewards)+1)])
    for i, state in enumerate(states):
        old_Q = Q[state][actions[i]] 
        old_N = N[state][actions[i]]
        Q[state][actions[i]] = old_Q + (sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)/(old_N+1)
        N[state][actions[i]] += 1
    
    return Q, N
```

```python
def mc_control_GLIE(env, num_episodes, gamma=1.0):
    nA = env.action_space.n
    # initialize empty dictionaries of arrays
    Q = defaultdict(lambda: np.zeros(nA))
    N = defaultdict(lambda: np.zeros(nA))
    # loop over episodes
    for i_episode in range(1, num_episodes+1):
        # monitor progress
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()
        
          # 設定epsilon值，隨著執行次數的增加，epsilon越來越小
        epsilon = 1.0 / ((i_episode/8000)+1)
            
        # 根據epsilon利用epsilon-greedy policy產生episode
        episode = generate_episode_from_Q(env, Q, epsilon, nA)
            
        # 利用此episode來更新action-value function
        Q, N = update_Q_GLIE(env, episode, Q, N, gamma)
            
    # 根據最後估測的action-value function來決定最好的policy
    policy = dict((k,np.argmax(v)) for k, v in Q.items())
        
    return policy, Q
```

<h2 id="18">MC控制: 常量a part1</h2>

- 假設如果已經有999次回報的平均值，當考慮第1000次回報，並不會對平均值有多大改變。

![615](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/615.png)

- 修改算法，採取是一個固定值，讓算法是平均重視每一次的回報。

![616](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/616.png)

<h2 id="19">MC控制: 常量a part2</h2>

 - 应该始终将 α 的值设为大于 0 并小于等于 1 之间的数字。
 	- 如果 α=0，则智能体始终不会更新动作值函数估算。
 	- 如果 α=1，则每个状态动作对的最终值估算始终等于智能体（访问该对后）最后体验的回报。
	- 如果 α 的值更小，则促使智能体在计算动作值函数估值时考虑更长的回报历史记录。
	- 增加 α 的值确保智能体更侧重于最近抽取的回报。

<h2 id="20">實現 - MC控制: 常量a</h2>

![617](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/617.png)


<h2 id="21">迷你項目 part4</h2>

```python
def update_Q_alpha(env, episode, Q, alpha, gamma):
    """ updates the action-value function estimate using the most recent episode """
    states, actions, rewards = zip(*episode)
    # 折扣率
    discounts = np.array([gamma**i for i in range(len(rewards)+1)])
    # 利用alpha來更新action-value function
    for i, state in enumerate(states):
        old_Q = Q[state][actions[i]] 
        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)
    return Q
```

```python
def mc_control_alpha(env, num_episodes, alpha, gamma=1.0):
    nA = env.action_space.n
    # initialize empty dictionary of arrays
    Q = defaultdict(lambda: np.zeros(nA))
    # loop over episodes
    for i_episode in range(1, num_episodes+1):
        # monitor progress
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()

        # 設定epsilon值，隨著執行次數的增加，epsilon越來越小
        epsilon = 1.0/((i_episode/8000)+1)
        
        # 根據epsilon利用epsilon-greedy policy產生episode
        episode = generate_episode_from_Q(env, Q, epsilon, nA)
        
        # 利用此episode來更新action-value function (使用常數alpha)
        Q = update_Q_alpha(env, episode, Q, alpha, gamma)
        
    # 根據最後估測的action-value function來決定最好的policy
    policy = dict((k,np.argmax(v)) for k, v in Q.items())
        
    return policy, Q
```

<h2 id="22">總結</h2>

- [迷你項目](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Jupyter/Monte_Carlo-zh.ipynb)



