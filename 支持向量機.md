# 支持向量機

## Agenda

- [簡介](#1)
- [距離最小化](#2)
- [誤差最小化](#3)
- [感知器算法](#4)
- [分類算法](#5)
- [邊界誤差](#6)
- [誤差函數](#7)
- [C參數](#8)
- [多項式內核](#9)
- [RBF核函數](#10)
- [sklearn 中的支持向量機](#11)

## Note

<h2 id="1">簡介</h2>

> - 分類算法。
> - 找出最佳的分割界線。

<h2 id="2">距離最小化</h2>

![132](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/132.png)

> - 左邊的分割線比右邊好。
> - 因為左邊的點到線的距離比右邊遠(表示穩定的切割數據)。

<h2 id="3">誤差最小化</h2>

**Margin最大化**

![133](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/133.png)

> - 分類問題，就是希望可以找到最好的界線。
> - 最好的界線，就是我界線Margin可以越大越好。

**誤差最小化**

![134](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/134.png)

> - 希望margin越大的同時，也希望分類誤差可以盡量越小。
> - 總誤差 = 分類誤差(margin外的誤差) + margin誤差

<h2 id="4">感知器算法</h2>

- 回顧之前學的感知器算法，把此算法當作最小誤差函數的算法。

![135](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/135.png)

> - 找出分類錯誤的點給予懲罰。
> - 分類錯誤的點，距離界線越遠，懲罰越大。

<h2 id="5">分類算法</h2>

- 分類算法跟感知器算法類似，只是多導入margin的分類錯誤的概念。

![136](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/136.png)

> - 分別以界線兩端的margin來找出分類錯誤的點給予懲罰。
> - 分類錯誤的點，距離對margin越遠，懲罰越大。

![137](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/137.png)

<h2 id="6">邊界誤差</h2>

**Margin Error**

- SVM的核心，不僅希望分類錯誤越低越好，另外還希望margin可以越大越好。
- 所以margin越小，就必須給的懲罰越大。
- 要如何量化margin跟懲罰之間的關係。

**量化公式**

![138](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/138.png)

**例子**

![139](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/139.png)

![140](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/140.png)

![141](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/141.png)

<h2 id="7">誤差函數</h2>

![142](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/142.png)

> - 目前學到的兩種錯誤:分類錯誤,邊界誤差。

![143](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/143.png)

> - 誤差函數 = 分類錯誤 + 邊界誤差。

<h2 id="8">C參數</h2>

- 透過C參數，讓我們的SVM的模型更加彈性。

![144](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/144.png)

> - 哪邊好 ？
> - 左邊，margin大，但有分類錯誤。
> - 右邊，margin小，但沒分類錯誤。
> - 沒有絕對哪邊好，根據模型的使用情境來決定，所以希望有個參數來調整模型，讓模型可以依據我們的需求學習左邊或右邊。

![145](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/145.png)

![146](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/146.png)

> - C越大，margin大，但分類錯誤越大。
> - C越小，margin小，但分類錯誤越小。

<h2 id="9">多項式內核</h2>

**一維數據的核函數切割**

![147](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/147.png)

> - 如何讓機器學習要怎樣分類此種數據分佈，沒辦法直接用一條線切。

![148](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/148.png)

> - **增加維度**，從一維(直線)變成二維(平面)。
> - 再利用**核函數**，將一維數據轉化二維數據
> - 在二維數據找出切割方式。
> - 最後將二維切割方式，轉換成一維的切割。

**二維數據的核函數切割**

![149](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/149.png)

> - 如何利用核函數來切割二維數據。

![150](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/150.png)

> - circle: 直接在平面上，畫一個圓來切割，犧牲了線性度(變複雜)。
> - building: 增加維度，在不同平面進行切割，犧牲了數據的維度(變複雜)。

![151](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/151.png)

> - 其實circle跟building是一體兩面的相同方式。

![152](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/152.png)

> - 這兩個kerenl可依據我們的資料屬性進行選擇。
> - 這些核函數的選擇，是屬於hyper parameter，可以通過訓練找出最佳核函數。 

<h2 id="10">RBF核函數</h2>

**RBF Kernel，跟之前學的linear kernel 或 polynomial kernel有什麼不同呢 ？**

![154](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/154.png)

> - RBF Kenel，可以做到上面這樣的切割方式。(似乎比linear kernel 或 polynomial kernel更加強悍)。

![155](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/155.png)

> - 透過三維空間的觀察，如何把二維空間的藍點提升到山頂，紅色在山底。
> - 透過一個水平切割面，進行分割。
> - 切割面跟山相交的部分，就是我們在二維空間中所需要的切割邊界。

![156](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/156.png)

> - 不同的權重設計，可以產生不同高度的山脈，讓大部分的藍點到高處，紅點在低處。

![157](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/157.png)

![158](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/158.png)

> - 超參數，可以幫助決定山脈陡峭程度(但不改變高度)。
> - 此參數越大，越準確，但也越容易overfitting。

**RBF背後的原理**

![153](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/153.png)

> - 建造出這個山脈函數，使得藍點剛好在山底，紅點剛好在山頂。

![159](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/159.png)

> - 透過不同權重單一山脈函數相加，來組合出我們所需要的山脈函數。

<h2 id="11">sklearn 中的支持向量機</h2>

**SVM中常見到的超參數**

![160](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/160.png)

```python
# Import statements 
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# Read the data.
data = np.asarray(pd.read_csv('data.csv', header=None))
# Assign the features to the variable X, and the labels to the variable y. 
X = data[:,0:2]
y = data[:,2]

# TODO: 构建支持向量机模型
# Find the right parameters for this model to achieve 100% accuracy on the dataset.
model = SVC(kernel='rbf', gamma = 27)

# TODO: 将模型与数据进行拟合
model.fit(X, y)

# TODO: 使用模型进行预测
y_pred = model.predict(X)

# TODO: 计算模型的准确率 
acc = accuracy_score(y_pred, y)

```

