# 線性迴歸

## Agenda

- [簡介](#1)
- [用數據擬合直線](#2)
- [調整一直線](#3)
- [絕對值技巧](#4)
- [平方技巧](#5)
- [梯度下降](#6)
- [平均絕對值誤差](#7)
- [平均平方誤差](#8)
- [最小誤差化函數](#9)
- [均方誤差與總平方誤差](#10)
- [小批量梯度下降法](#11)
- [絕對值誤差 VS 平方誤差](#12)
- [scikit-learn中的線性迴歸](#13)
- [高維度](#14)
- [多元線性回歸](#15)
- [解數學方程組](#16)
- [線性迴歸注意事項](#17)
- [多項式回歸](#18)
- [正則化](#19)
- [神經網路回歸](#20)
- [嘗試神經網路](#21)

## Note

<h2 id="1">簡介</h2>

> - **分類**: 預測一個狀態。
> - **回歸**: 預測一個值。
> - 本章會先從線性回歸討論起，然後再學習如何處理非線性迴歸。

<h2 id="2">用數據擬合直線</h2>

![35](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/35.png)

> - 每一次，都慢慢地調整直線，讓直線可以儘可能地靠近所有的點。

<h2 id="3">調整一直線</h2>

![36](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/36.png)

> - W1增大: 逆時針轉動。
> - W1減小: 順時針轉動。
> - W2增大: 平行上移。
> - W2減小: 平行下移。

<h2 id="4">絕對值技巧</h2>

![37](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/37.png)

> - 利用p值，來調整直線，讓直線來更靠近點。
> - learning rate: 控制直線的移動步伐。

![38](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/39.png)

![39](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/39.png)

<h2 id="5">平方技巧</h2>

![40](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/40.png)

> - 跟絕對值技巧不同的地方是，如果點離直線近，那就慢慢調整直線，如果點離直線遠，那調整直線的幅度就增大。
> - 不只利用p值，還利用了(q-y)高度差。

<h2 id="6">梯度下降</h2>

![41](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/41.png)

> - 紅色線:Weight所構成的error function。
> - 綠色線:error function對於weight所偏微分，也就是梯度下降的方向。
> - 利用偏微分的結果來更新weight，讓整體error值下降。


<h2 id="7">平均絕對值誤差</h2>

![42](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/42.png)

> - 在梯度下降中，有提到所謂的error function，在Linear regression用到的error function有兩種。
> 
> > - Mean absoulte error 平均絕對值誤差。
> > - Mean square error 平均平方誤差。

<h2 id="8">平均平方誤差</h2>

![43](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/43.png)

<h2 id="9">最小誤差化函數</h2>

![44](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/44.png)

> - 到目前為止，我們學到兩種方式來讓直線來擬合這些點。
> 
> > - Trick: 絕對值技巧, 平方技巧
> > - Error functions: 平均絕對值誤差, 平均平方誤差
> 
> - 其實Trick跟Error functions是相對應的方式。


![45](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/45.png)

![47](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/47.png)

> - 推導證明：


![46](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/46.png)

![48](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/48.png)

> - 推導證明：

<h2 id="10">均方誤差與總平方誤差</h2>

![49](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/49.png)

<h2 id="11">小批量梯度下降法</h2>

![50](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/50.png)

![51](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/51.png)

> - Batch:同時依據所有點進行調整。
> - Stochastic: 一次只依據一個點進行調整。
> - Mini-batch: 一次只依據部分的點進行調整。

<h2 id="12">絕對值誤差 VS 平方誤差</h2>

> - 什麼時候該用絕對值誤差？ 什麼時候該用平方誤差

![52](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/52.png)

> - A,B,C三條線，斜率相同，只是上下平移。
> 
> > - A,B,C具有相同絕對值誤差。
> > - B具有最小平方誤差。(因爲平方誤差是二次函數，會有最小值)

![53](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/53.png)

<h2 id="13">scikit-learn中的線性迴歸</h2>

**使用关于全球男性平均出生预期寿命和平均 BMI 的数据**

```python
# TODO: Add import statements
import pandas as pd
from sklearn.linear_model import LinearRegression

# Assign the dataframe to this variable.
# TODO: Load the data
bmi_life_data = pd.read_csv("bmi_and_life_expectancy.csv")
 
# Make and fit the linear regression model
#TODO: Fit the model and Assign it to bmi_life_model
bmi_life_model = LinearRegression()
bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])

# Make a prediction using the model
# TODO: Predict life expectancy for a BMI value of 21.07931
laos_life_exp = bmi_life_model.predict(21.07931)
```

<h2 id="14">高維度</h2>

![54](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/54.png)

> - 之前討論都是在一維空間，現在延伸到高維空間。
> - 一樣可以用之前學到的技巧(絕對值技巧, 平方技巧)，Error functions(平均絕對值誤差, 平均平方誤差)，來找出最佳的擬合結果。


<h2 id="15">多元線性回歸</h2>

**使用 波士顿房价数据集。该数据集包含 506 座房子的 13 个特征，均值为 $1000's。你将用一个模型拟合这 13 个特征，以预测房价**

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# Load the data from the boston house-prices dataset 
boston_data = load_boston()
x = boston_data['data']
y = boston_data['target']

# Make and fit the linear regression model
# TODO: Fit the model and assign it to the model variable
model = LinearRegression()
model.fit(x, y)

# Make a prediction using the model
sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,
                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,
                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]
# TODO: Predict housing price for the sample_house
prediction = model.predict(sample_house)
```

<h2 id="16">解數學方程組</h2>


<h2 id="17">線性迴歸注意事項</h2>

**线性回归隐含一系列前提假设，并非适合所有情形，因此应当注意以下两个问题。**

> - 最适用于线性数据
> 
> > - 线性回归会根据训练数据生成直线模型。如果训练数据包含非线性关系，你需要选择：调整数据（进行数据转换）、增加特征数量（参考下节内容）或改用其他模型。

![55](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/55.png)

> - 容易受到异常值影响
> 
> > - 线性回归的目标是求取对训练数据而言的 “最优拟合” 直线。如果数据集中存在不符合总体规律的异常值，最终结果将会存在不小偏差。

![56](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/56.png)

<h2 id="18">多項式回歸</h2>

![57](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/57.png)

> - 單純的直線無法很好擬合這些點，所以改用高次多項式。
> - 例如，我們要解決這4個權重(找出最佳的權重值)
> 
> > - 先求出平方誤差的平均值。
> > - 分別對4的變量進行偏微分。
> > - 利用梯度下降法，修改這4個權重，找出最小化的誤差。

<h2 id="19">正則化</h2>

**正则化的目的是限制参数过多或者过大，避免模型更加复杂。**

![58](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/58.png)

> - 除了考慮模型的準確性外，也要考慮模型的複雜性，太複雜的模型有可能導致overfitting。

![59](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/59.png)

> - L1 regularization

![60](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/60.png)

> - L2 regularization

![61](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/61.png)

![62](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/62.png)

> - large lamba: 模型會傾向簡化。
> - small lamba: 模型會傾向複雜化。

![63](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/63.png)

> - 這段沒有很懂，講得太不清楚了 ？？


<h2 id="20">神經網路回歸</h2>

![64](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/64.png)

> - 另外一種擬合的方式，拆成多個直線段來進行擬合(不一定要用高維多項式)。

![65](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/65.png)

> - 神經網路回歸的重點，就是將最後一個sigmoid的單元刪除，如此就是得到前面各層的輸出加權總和(從分類就變成回歸)。

![66](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/66.png)

> - 如何利用神經網路回歸來進行多個直線段的擬合。
> 
> > - 每個單元都是單純的線性組合，並在透過relu進行輸出。
> > - 多層layer的疊加後，就會擬合成多個直線段。

<h2 id="21">嘗試神經網路</h2>

> - 神奇的神经网络 “游乐场”，在这里你可以看到很棒的可视化效果，并可以使用参数来解决线性回归问题，然后尝试一些神经网络回归

[游乐场](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)

