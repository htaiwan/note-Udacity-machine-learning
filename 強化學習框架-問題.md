# 強化學習框架-問題

## Agenda

- [設置-回顧](#1)
- [階段性任務與連續性任務](#2)
- [獎勵假設](#3)
- [目標和獎勵](#4)
- [獎勵累積](#5)
- [折扣回報](#6)
- [MDP](#7)
- [有限MDP](#8)
- [總結](#9)

## Note

<h2 id="1">設置-回顧</h2>

- **Agent**通過不斷的學習跟遇到錯誤，學習如何在**環境**中完成各項**動作**並最大化**獎勵**。

- Agent, 環境, 動作, 獎勵 - 整合在一個框架中。

![470](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/470.png)

- 學習過程。
	- 環境更新狀態給Agent
	- Agent收到狀態，更新動作給環境。
	- 環境收到動作，更新狀態跟獎勵給Agent。
	- Agent收到狀態跟獎勵，更新動作給環境。

![473](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/473.png)

![471](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/471.png)

![472](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/472.png)

- 目標: 最大化累積獎勵。

![474](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/474.png)


<h2 id="2">階段性任務與連續性任務</h2>

- 階段性任務 - 具有清晰結束點
	- ex. 玩遊戲過關，無人車的撞毀
	- 每次Agent結束其任務，開始下次任務時，都會掌握前生所學習到一些知識，透過不斷的重複任務來學習知識。
	
![475](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/475.png)

- 連續性任務 - 不具有清晰結束點
	- ex. 金融市場的股票買入跟賣出。
	- 需要學習選擇動作的最佳方式並不斷與環境互動。(此算法更複雜)

![476](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/476.png)

<h2 id="3">獎勵假設</h2>

- 不同的任務，都有不同的目標。
	- 這些看起來似乎都是不同的目標，都有一個一致性的原則。

![477](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/477.png)

- 獎勵假設:所有目標的一致性原則-**最大化期望累積獎勵。**

![478](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/478.png)

<h2 id="4">目標和獎勵</h2>

- 以機器人走路為例子，來討論強化學習框架中，**action, state跟reward應該是什麼**

![479](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/479.png)

- **Action**
	- 機器人在哪些關節點所使用的力量。

![480](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/480.png)

- **State**
	- 所有關節點當前的位置跟速度。
	- 機器人所站的地面的量測結果。
		- 地面的平坦度，道路上是否有階梯。
	- 接觸傳感器數據。
		- 機器人是否在行走，或者跌倒。

![481](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/481.png)

- 機器人必須根據當前的狀態，來規劃下一個動作。

- **Reward**
	- walk fast(走的速度越快越好)。
	- walk forward(走的方向是往前，而不是方向亂變)。
	- walk smoothly(走的姿勢是否很怪異，關節點力量太大)。
	- walk for long as possible(走得越久越好，不要馬上跌倒)。 

![482](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/482.png)

- 機器人學走路的目標就是最大化累積獎勵。

<h2 id="5">獎勵累積</h2>

- **強化學習框架的回顧:**
	- 智能體如何通過與環境的互動實現目標。
	- 互動簡化成三種訊號。
	- 狀態訊號: 環境向智能體呈現狀況的方式。
	- 動作訊號: 智能體做出動作來影響環境。
	- 獎勵訊號: 環境針對智能體做出正確動作給予回饋。
	- 目標: 最大化累積獎勵。

![483](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/483.png)

- 如何最大化累積獎勵 ?
	- 不能著重單個時間步 - 只會學習到最大初始化的獎勵動作。
		- ex. 機器人只會走得快，在短時間不跌倒，但會走不穩，走不對方向。
	- 動作有分成短期後果跟長期後果。
	- 智能體要考慮所有時間步的獎勵。


- 獎勵:
	- 過去的獎勵是無法改變。
	- 只有未來的獎勵能受到控制。

![484](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/484.png)

- 因為智能體無法完全肯定預測未來獎勵如何，他必須依賴預測或估算。

![485](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/485.png)

<h2 id="6">折扣回報</h2>

- 當前獎勵權重跟未來獎勵權重該相同嗎？

- 折扣回報: 更關心近期獎勵，而不是遙遠未來的獎勵。

![486](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/486.png)

- 折扣率越小，越關心近期的獎勵，越大，在越關心未來的獎勵。
- 使用折扣來避免無限未來產生的不良影響。

![487](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/487.png)


<h2 id="7">MDP</h2>

- 目標:讓回收機器人如何考慮電量狀況下，完成收拾越多回收的垃圾。

- Actions:
	- search: 房間中搜尋垃圾。
	- recharge: 回充電站充電。
	- wait: 保持不動，讓別人直接丟垃圾進去。

![488](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/488.png)

- States:
	- high: 電量高。
	- low: 電量低。

![489](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/489.png)

- 流程1: 
	- 高電量時，機器人決定進行search，其結果有可能是70%機會仍保持高電量，30%機會變成低電量，因為積極搜尋所以給予4個獎勵。

![490](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/490.png)

- 流程2: 
	- 高電量時，機器人決定進行wait，其結果是100%機會仍保持高電量，因為等待所以只給予1個獎勵。

![491](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/491.png)

- 流程3: 
	- 低電量時，機器人決定進行wait，其結果是100%機會仍保持低電量，因為等待所以只給予1個獎勵。

![492](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/492.png)

- 流程4: 
	- 低電量時，機器人決定進行recharge，其結果是100%機會變回高電量，因為充電所以不給予獎勵。

![493](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/493.png)

- 流程5: 
	- 低電量時，機器人決定進行search，其結果是80%機會耗低電量，死在搜尋過程中，即管是積極搜尋，仍給予3個懲罰，因為20%機會會躲過這個風險，因為積極搜尋所以給予4個獎勵。。

![494](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/494.png)

- **動態特性**
	- 環境是如何根據智能體的動作來動態決定狀態跟獎勵。 

![496](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/496.png)

![497](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/497.png)

- **MDP回顧**
	- Markov Decision Process
	- 狀態集合，動作集合，獎勵集合，環境動態特性和折扣率所決定。
	- 一般折扣率的設定，都會接近1，而不是接近0，否則智能體只會著重近的未來，而非長期未來。
	- 智能體並不清楚獎勵集合跟環境動態特性，是透過互動學習來學習如何達到目標。

![495](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/495.png)

<h2 id="8">有限MDP</h2>

![498](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/498.png)

- [OpenAI Gym 中的可用环境](https://github.com/openai/gym/wiki/Table-of-environments)

<h2 id="9">總結</h2>

- 強化學習中智能體與環境的互動

![499](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/499.png)

- 设置，重新经历

![500](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/500.png)

- 阶段性任务与连续性任务

![501](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/501.png)

- 奖励假设

![502](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/502.png)

- 目标和奖励

![503](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/503.png)

- 累积奖励

![504](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/504.png)

- 折扣回报

![505](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/505.png)

- MDPs和一步动态特性

![506](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/506.png)
