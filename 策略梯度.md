# 策略梯度

## Agenda

- [基於策略的方法](#1)
- [為何要使用基於策略的方法](#2)
- [策略函數逼近](#3)
- [隨機性策略探索](#4)
- [策略梯度](#5)
- [蒙地卡羅策略梯度](#6)
- [受限策略梯度](#7)
- [總結](#8)


## Note

<h2 id="1">基於策略的方法</h2>

- 強化學習的最終目的就是學習到最優策略。
- 目前為止，學到各種基於值的方法來嘗試找到最優的值函數，然後再間接透過這個值函數來定義策略(例如可以使用Epsilon貪婪動作選擇法)。
- **是否可以直接得到最佳策略，而跟不用管值函數??**


<h2 id="2">為何要使用基於策略的方法</h2>

- 主要原因
	- 簡單性
	- 隨機性策略
	- 連續動作空間

![723](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/723.png)

**簡單性**
	
- 在Q學習中，我們使用了值函數的概念，做為找出最優策略的中間步驟，但如果目標是找出最優策略，可以直接估算最優策略嗎？

![724](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/724.png)
	
- Policy
	- 如果是是確定性方法，則該策略只需要從狀態映射到動作。
	- 如果是隨機性方法，則為每個狀態下每個動作的條件機率。
	- 由於狀態空間大部分可能具有相同的值，以這種方式構建策略使我們能夠可能的時候進行此類泛化，並著重狀態空間中更複雜的區域。

![725](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/725.png)

	
**隨機性策略**

- 學到真正的隨機性策略。
- 當使用Epsilon貪婪動作選擇法，如果Epsilon大於某個值，就遵守確定性策略，小於某個值就採用隨機性選擇，但底層值函數仍然會促使我們遠則特定動作。

![726](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/726.png)

- Aliased State
	- 擁有相同特徵的狀態空間。
	- 因為具有相同特徵，所以會有相對值函數，也導致會有相同的動作選擇。
	- 在下面的例子，當Agent走到兩塊問號方格，會有相同相同的動作選擇，那麼Agent很有可能被困在左上方的兩個方格中出不來，雖然透過Epsilon也許有機會跳脫出來，但會相當耗時。
	- 正確作法，是這些Aliased State是具有相同機率的隨機選擇動作。

![727](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/727.png)

![728](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/728.png)

![729](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/729.png)

	
**連續動作空間**

- 如果動作空間是連續，尤其該函數非凸，那就很難決定最佳動作，尤其更高維度的動作空間中會更複雜。
- 如果可以將特定狀態直接映射到動作，即使生成的策略有點複雜，但可以顯著所需的計算時間。

![730](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/730.png)

<h2 id="3">策略函數逼近</h2>

- 就像值函數逼近一樣，我們用一組參數theta來參數化策略。我們的目標就變成是優化這些參數，找出最佳策略。

![731](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/731.png)

**Linear function with Softmax Policy**

- 一個簡單的近似函數就是這些特徵進行線性組合。
- softmax會進行指數化產生0~1，在將這些指數化的線性組合，進行標準化。
- 這樣會形成一組合為1的動作機率值。
- 這樣的轉換僅適用一組離散的動作空間。

![732](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/732.png)

**Linear function with Gaussian Policy**

- 當環境具有連續動作空間時，使用高斯策略，即從高斯或正態分佈中抽取動作

![733](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/733.png)

- **Objective Function**
	- 如何判斷哪個策略是最佳策略？
	- 利用一個函數來計算使用該策略時獲得獎勵的預期值

![734](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/734.png)

**State Values**

- 階段性任務：可以利用第一個時間步G1的回報均值。
- 連續性任務：對於每個狀態以及對應的發生機率進行加權。
	- 計算方式: 就是將狀態發生次數/所有狀態的總發生次數

![735](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/735.png)

**Action Values**

- 計算平均的動作值或Q值。

![736](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/736.png)

**Reward Values**

- 計算每個時間步的平均獎勵。

![737](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/737.png)

- 總結: 這些公式在優化策略方面都表現得不錯，選擇哪個目標，可以根據狀態選擇最方便計算的方式。

![738](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/738.png)


<h2 id="4">隨機性策略探索</h2>

- 爬山法：沿著目標函數表面走動，直到山丘頂部。

![739](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/739.png)

**最陡爬山法**

- 隨機選擇一個策略後，然後根據當前策略的附近狀況，選擇一個最理想的策略進行迭代。

![740](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/740.png)

**模擬退火法**

- 利用預定義的計畫表，控制如何探索策略空間，一開始就搜索的臨近區域很廣泛，然後逐漸收小搜索區域。
- 有點像退火跟烙鐵，通過鐵塊加熱後逐漸冷卻，使鐵分子可以進入最佳結構。

![741](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/741.png)

**縮放法**

- 跟模擬退火法很類似，但當找不到最更好的策略時，就要調大搜索區域。

![742](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/742.png)


<h2 id="5">策略梯度</h2>

- 隨機性策略並非一定可已找到最佳的結果，有可能陷入局部最佳或需要長時間才會收斂。但優點就是不需要了解目標函數，就是把目標函數當作一個黑盒子，傳入input得到output，根據output做合理推測下一步該如何進行。

**策略梯度**

- 如果能計算目標函數，就可以讓梯度始終指向最大的變化方向。

![743](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/743.png)


**有限差估算梯度**

- 當梯度是無法微分時，改用有限差估算梯度

![744](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/744.png)

**策略梯度分析**

- 有點難懂，之後再回來深入研究。

![745](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/745.png)

![746](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/746.png)

![747](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/747.png)


<h2 id="6">蒙地卡羅策略梯度</h2>

**強化算法**

- 對於每個階段(通過策略pi產生的軌跡)
- 對於該階段中的每個時間步t
- 計算策略函數生成的對數機率的梯度乘上該階段剩餘時間步的回報
- 再利用很小的學習速率更新權重

![748](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/748.png)


<h2 id="7">受限策略梯度</h2>

- 我們不關心策略是採取什麼動作，經歷什麼狀態，或者頻率有多大等等。我們只關心整體的獎勵。
- 這樣有可能形成再目標函數空間中非常接近，但是行為截然不同的兩個策略。

![749](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/749.png)

**Intermediate Policy**

- 假設Policy B是比Polcy A好，則我們的優化算法則是嘗試更新參數從A到B。
- 透過一組中間策略參數跟學習率alpha，來進行策略改善。
- 但如果顯著改變策略參數可能會導致行為明顯改變。
- 除了目標函數外，應該也要注意策略參數。

![750](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/750.png)

**方式一: Constrained Policy**

- 要求兩個策略之間差異不能超過某個值。

![751](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/751.png)

**方式二: Penalty Term**

- 向目標函數增加懲罰項，有點像機器學習的正規化。

![752](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/752.png)

- 無論是方式一或二都需要某種方式來量化兩個策略之間的差異。

**KL-Divergence**

- 兩個策略P, Q，如何比較這兩個差異。

![753](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/753.png)

- KL散度

![754](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/754.png)

![755](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/755.png)

![756](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/756.png)

![757](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/757.png)

- 計算當前策略theta與任何新候選策略theta'之間的KL散度，然後使用這個差異項作為目標函數的限制條件，該項會懲罰使行為變化很大的新策略，因此新策略必須帶來更多價值，才能被視為更好的策略。

![758](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/758.png)


<h2 id="8">總結</h2>

- 策略梯度是一個非常強大強化學習系列之一。
- 可以直接映射狀態到動作，不需要再處理值函數。
- 可以學習連續性任務，在這種任務中，動作是連續項而非離散。
- 可以獲的真實的隨機性策略。

![759](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/759.png)