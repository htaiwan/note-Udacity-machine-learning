# 決策樹

## Agenda

- [簡介](#1)
- [推薦應用](#2)
- [學生錄取資格](#3)
- [熵](#4)
- [熵公式](#5)
- [信息增益](#6)
- [信息增益最大化](#7)
- [隨機森林](#8)
- [超參數](#9)
- [sklearn 中的決策樹](#10)
- [使用决策树探索泰坦尼克乘客存活模型](#11)

## Note

<h2 id="1">簡介</h2>


<h2 id="2">推薦應用</h2>

![87](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/87.png)

> - 如何讓machine來學習構建這樣的決策樹? 
> - 哪個feature(occupation, gender)在何時需要被抽取出來當作決策?

<h2 id="3">學生錄取資格</h2>

![88](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/88.png)

> - 在看另外一個例子，如何用決策樹來解決之前說的的學生錄取問題。
> - 利用多個直線段一一切割(x=5, y=7, y=2)，再來構建出決策樹。

<h2 id="4">熵</h2>

**Entropy**

- 是一個物理概念。

![89](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/89.png)

> - Entropy可以精準測量一個粒子的自由程度。
> - 水蒸氣的Entropy最高，冰的Entropy最低。


- 用機率來解釋。

![90](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/90.png)

> - 最左邊的桶子，不管我們怎樣重新擺放球的位置，都是一樣的結果，所以Entropy最低。
> - 最右邊的桶子，隨著重新擺放球的位置，擁有多種的可能性，所以Entropy最高。

**集合越穩固或越具有同類性其Entropy越低**


- 用知識來解釋。

![91](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/91.png)

> - 最左邊的桶子，我們非常確定球一定是紅色，所以是high knowledge，對應的Entropy最低。
> - 最右邊的桶子，確定球是紅色越低，所以是low knowledge，對應的Entropy最高。


<h2 id="5">熵公式</h2>

![92](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/92.png)

> - 從每個桶子都抽出4顆球(抽完都放回去)，其顏色組合跟桶子內的球原本的顏色組合一樣就贏。
> - 最左邊的桶子，贏的機率最高。
> - 最右邊的桶子，贏的機率最低。


![93](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/93.png)

**利用log函數讓這些機率乘積改成和，方便計算**

![94](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/94.png)

> - 最後一行就是我們的Entropy。

![95](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/95.png)

> - 多幾顆球的Entropy計算例子。

![96](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/96.png)

> - 熵公式(公式“m-n" 应该为"m+n")。

<h2 id="6">信息增益</h2>

**Information Gain**

![97](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/97.png)

> - Information Gain就是將**"老爸的Entropy減掉兒子們的Entropy平均"**。

![98](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/98.png)

> - 決策樹在構建過程中，會根據Information Gain來決定如何構建，在這個例子決策樹會選擇最右邊的構建方式。

<h2 id="7">信息增益最大化</h2>

![99](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/99.png)

> - 先算出根節點的Entropy(3個pokemon, 2個whatapp, 1個snapchat)。

![100](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/100.png)

![101](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/101.png)

![102](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/102.png)

> - 分別計算出以occupation跟gender所得到information gain。
> - 決定會先從gender(information gain高)開始建構決策樹。

<h2 id="8">隨機森林</h2>

![103](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/103.png)

![104](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/104.png)

> - 隨著特徵數據量的變多，決策樹很容易遇到overfitting的問題。

![105](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/105.png)

> - **隨機森林**: 利用表中**隨機**挑選不同的特徵值來構建**多個決策樹**。
> - 讓多個決策樹同時進行決策，並選取結果中顯示最多的。


<h2 id="9">超參數</h2>

- 最大深度(max_depth)
- 每片叶子的最小样本数(min_samples_leaf)
 	- 在分裂节点时，很有可能一片叶子上有 99 个样本，而另一片叶子上只有 1 个样本。这将使我们陷入困境，并造成资源和时间的浪费。如果想避免这种问题，我们可以设置每片叶子允许的最小样本数。
 	- 这个数字可以被指定为一个整数，也可以是一个浮点数。如果它是整数，它将表示这片叶子上的最小样本数。如果它是个浮点数，它将被视作每片叶子上的最小样本比例。比如，0.1 或 10% 表示如果一片叶子上的样本数量小于该节点中样本数量的 10%，这种分裂将不被允许。
- 每次分裂的最小样本数(min_samples_split)
	- 这个参数与每片叶子上的最小样本树相同，只不过是应用在节点的分裂当中。
- 最大特征数(max_features)
	- 限制每个分裂中查找的特征数。如果这个数字足够庞大，我们很有可能在查找的特征中找到良好特征（尽管也许并不是完美特征）。然而，如果这个数字小于特征数，这将极大加快我们的计算速度。

<h2 id="10">sklearn 中的決策樹</h2>

```python
# Import statements 
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# Read the data.
data = np.asarray(pd.read_csv('data.csv', header=None))
# Assign the features to the variable X, and the labels to the variable y. 
X = data[:,0:2]
y = data[:,2]

# TODO: Create the decision tree model and assign it to the variable model.
# You won't need to, but if you'd like, play with hyperparameters such
# as max_depth and min_samples_leaf and see what they do to the decision
# boundary.
# 如果不設置超參數，默认的超参数将以 100% 的准确率拟合数据
# 盡量調整这些超参数，例如 max_depth 和 min_samples_leaf
# 并尝试找到最简单的潜在模型，不要太过拟合模型！
model = DecisionTreeClassifier()
# DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)

# TODO: Fit the model.
model.fit(X, y)

# TODO: Make predictions. Store them in the variable y_pred.
y_pred = model.predict(X)
```

<h2 id="11">使用决策树探索泰坦尼克乘客存活模型</h2>

[使用决策树探索泰坦尼克乘客存活模型](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Jupyter/titanic_survival_exploration-zh.ipynb)


