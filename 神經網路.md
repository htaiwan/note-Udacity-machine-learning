# 神經網路

## Agenda

- [簡介](#1)
- [分類問題](#2)
- [線性界線](#3)
- [更高維度](#4)
- [感知器](#5)
- [為何是神經網路](#6)
- [感知器算法](#7)
- [非線性數據](#8)
- [誤差函數](#9)
- [對數損失誤差函數](#10)
- [離散型與連續型](#11)
- [Softmax函數](#12)
- [One-Hot編碼](#13)
- [最大似然率](#14)
- [交叉熵](#15)
- [多類別的交叉熵](#16)
- [Logistic回歸](#17)
- [梯度下降](#18)
- [梯度下降算法](#19)
- [感知器和梯度下降](#20)

## Note

<h2 id="1">簡介</h2>

- 深度學習的基礎就是神經網路。
- 神經網路可以幫助我們找出數據中的最佳分割線，來區分不同的數據。

<h2 id="2">分類問題</h2>

- 機器如何找出這條分割線 ?

![195](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/195.png)

<h2 id="3">線性界線</h2>

- 這個方程式就是我們的model，也就是機器都過數據分析，學習到的最終結果。

![196](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/196.png)

- 一般化的表示。

![197](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/197.png)

<h2 id="4">更高維度</h2>

- 三維數據。

![198](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/198.png)

- 高維數據。

![199](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/199.png)

<h2 id="5">感知器</h2>

- 感知器(perception): 就是將原本方程式的架構，轉化成神經元結構，有輸入神經元，運算神經元。

![200](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/200.png)

- 權重(weight): 輸入神經元的權重。

![201](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/201.png)

- 高維數據的感知器。

![202](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/202.png)

- 階躍函數(stpe function): 將運算神經元所輸出的數值轉化成1或0。

![203](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/203.png)

<h2 id="6">為何是神經網路</h2>

- perception和神經系統的對比
	- Dendrites: 輸入神經元。
	- Neucleus: 運算神經元。
	- Axon: 也許是step function。 

![204](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/204.png)

<h2 id="7">感知器算法</h2>

- 我們一開始的問題是，如何找出這條直線。
- 答案: 透過感知器算法可以讓機器找出這條直線。
- **感知器算法**
	- 1. 一開始先隨機設定權重 w,b。(所以隨機決定一條分割線)
	- 2. 對於那些分錯的點：
		- 如果是藍色的點分錯:
			- 增加權重
		- 如果是紅色的點分錯：
			- 減少權重
	- 3. 調整完分割線後，再重複step2，直到下列條件後停止。
		- 直到沒有分錯的點。
		- 或者分錯的點在我們容許範圍內。
		- 或者重複到K次。

![205](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/205.png)

<h2 id="8">非線性數據</h2>

- 針對非線性數據處理，我們需要重新定義感知器算法，使其可以支援除了直線以外的其他類型曲線。

![206](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/206.png)

<h2 id="9">誤差函數</h2>

- 要處理非線性數據的第一步，就是要借助**誤差函數**。
- **誤差函數**: 讓我們可以了解與正確答案的差距有多大。
	- 透過重新定義感知器算法，不斷的修正與計算誤差函數，想辦法降低誤差函數的大小。

<h2 id="10">對數損失誤差函數</h2>

- 先了解離散型跟連續型數據對誤差函數的影響:
	- 離散型(Discrete): 很難透觀察到誤差函數改變。
	- 連續型(Continous): 很容易觀察到誤差函數改變。

![207](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/207.png)

- 例子: 若把誤差函數定義成分錯的個數，那此時誤差函數是針對離散型數據設計。
	- 會發生每次稍微調整直線變化，所得到誤差函數的結果都不會改變。
	- **正確做法: 要想辦法誤差函數的輸出從離散型變成連續型。**
		- 誤差函數是一個可微分的函數。
- 接下來目標: 如何設計可微分的誤差函數。

![208](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/208.png)

![209](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/209.png)

<h2 id="11">離散型與連續型</h2>

- 從預測角度來解釋誤差函數的輸出是離散型或連續型。
	- 離散型(Discrete): 就是YES,NO。
	- 連續型(Continous): 就是機率。
	
![210](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/210.png)

- 再看一下，之前所舉的例子。
	- 離散型(Discrete): 表示我們預測每個點不是藍色就是紅色。
	- 連續型(Continous): 表示我們預測每個點是藍色或是紅色的機率。

![211](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/211.png)

- 要如何從離散型變成連續型 ?
	- **使用了不同Activation function**

![212](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/212.png)

- 最後，如何調整感知器算法 ？
	- **使用了不同Activation function**

![213](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/213.png)


<h2 id="12">Softmax函數</h2>

- 多分類問題: 假設現在有多個動物特徵的輸入值，我們要來預測分別看到這三種動物的機率各是如何?
	- 機率總和要為1。

![214](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/214.png)

- 根據特徵值給評分: 各動物的評分 / 加總評分
	- 問題: 線性函數有可能遇到評分為負數的狀況。
	- 解決: 不要讓評分有負數的情況。

![215](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/215.png)

- 使用exp函數: 保證不會有負數。

![216](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/216.png)

- Softmax函數: 用來處理多分類問題的機率。
	- 當Softmax遇到二分類問題，其效果就等於sigmoid函數。

![217](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/217.png)

<h2 id="13">One-Hot編碼</h2>

- 常用的資料預處理方式。
- 透過這樣的方式將不同分類標示成不同數值。

![218](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/218.png)

<h2 id="14">最大似然率</h2>

- Maximum Likelihood。
	- 目標是找尋一個最佳的演算法，來幫助我們選擇那些可以正確區份數據的model。
	- 透過不同的model的最大似然率來代表其model的效能。
- 假設一個模型的切割方式如下圖，那麼藍色區域的點是藍色的機率比較大，相對的紅色區域的點是紅色的機率比較大。

![219](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/219.png)

- 兩個不同模型的最大似然率的計算方式(獨立機率相乘)。
	- 算出來的最大似然率顯然右邊的模型優於左邊。
	- 模型整體的最大似然率越高，表示此模型的效能越好。

![221](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/221.png)

<h2 id="15">交叉熵</h2>

- **Cross Entropy**
	- 是從最大似然率轉化而來的。
		- 將最大似然率先**取對數**，在取**負數**。
	
![222](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/222.png)

- 將最大似然率轉轉成Cross Entropy，可以發現錯誤的點的Cross Entropy較高。
	- 目標: 模型整體的Cross Entropy越低，表示此模型的效能越好。

![223](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/223.png)

- 公式推導:
	- P1,P2,P3: 表示不同門後有禮物的機率分別是多少。
	- yi: 表示第i個門後實際是否有禮物。
	- CE[(1,1,0), (0.8, 0.7, 0.1)] = 0.69
		- 根據我們所知的機率，出現1,1,0這樣的實際結果，是相當大的。 
	- CE[(0,0,1), (0.8, 0.7, 0.1)] = 0.69
		- 根據我們所知的機率，出現0,0,1這樣的實際結果，是相當小的。 

![224](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/224.png)


<h2 id="16">多類別的交叉熵</h2>

- 知道不同動物出現在不同門後的機率，可以算出實際這樣動物排列出現在門後的機率。

![226](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/226.png)

- yij = 1, 表示動物i實際出現在門j後。
- 所以整個公式多了一個m，m表示類別數量。
	- 當m=2時，公式結果會跟上面的一樣。

![225](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/225.png)

<h2 id="17">Logistic回歸</h2>

- 如何從Cross Entropy來推導出Error function的公式該如何寫 ？
	- y=1, 實際是藍點。
	- y^=0.6, 模型預測出是藍點的機率。
	- error = -ln(y^), 將機率轉換成 Cross Entropy。
- Error的公式: 算出每個點的Cross Entropy。
- Error Function的公式: 加總每個每個點的Cross Entropy，再取平均。

![227](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/227.png)

- 將Error Function的y^，轉換成x,b。

![228](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/228.png)

- 多分類的Error Function。

![229](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/229.png)

<h2 id="18">梯度下降</h2>

- 目標: 要根據算出來的梯度走相反方向。

![230](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/230.png)

- 導入learning rate，讓我們可以控制前進的步伐。

![231](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/231.png)

- [梯度下降公式推導]()

<h2 id="19">梯度下降算法</h2>

- 隨機選取權重，針對每一個點更新權重，直到error小到一定程度。
- 跟感知器算法很類似。

![232](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/232.png)

<h2 id="20">感知器和梯度下降</h2>

- 感知器
	- 只針對分類錯誤的點，進行調整。
		- 讓邊界可以靠近錯誤的點。
	- y^ 只能是0,1。
- 梯度下降
	- 對所有的點，進行調整。
		- 讓邊界可以靠近錯誤的點。(希望可以錯得越少越好)
		- 讓邊界可以遠離正確的點。(希望可以越正確越好)
	- y^ 可以是任何值。

![233](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/233.png)

![234](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/234.png)