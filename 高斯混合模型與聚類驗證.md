# 高斯混合模型與聚類驗證

## Agenda

- [高斯混合模型](#1)
- [一維高斯分佈](#2)
- [一維高斯混合模型](#3)
- [二維高斯分佈](#4)
- [二維高斯混合模型](#5)
- [期望最大值算法](#6)
- [期望最大值例子](#7)
- [GMM實現](#8)
- [GMM優缺點](#9)
- [聚類分析過程](#10)
- [聚類驗證](#11)
- [外部評價指標](#12)
- [內部評價指標](#13)

## Note

<h2 id="1">高斯混合模型</h2>

- **Gaussian Mixture Model Clustering (GMM)**: 假設每個cluster都遵循特定的統計分佈，利用概率跟統計的方式來找出這些cluster。

- **Normal Distribution**: 高斯分佈，現實中的許多數據集都是近似這樣的分佈 ex.考試成績，人的身高..。

![405](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/405.png)


<h2 id="2">一維高斯分佈</h2>

- **平均值，標準差，分布區域**
- **68%, 95%, 99%**

![406](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/406.png)

![407](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/407.png)

![408](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/408.png)

<h2 id="3">一維高斯混合模型</h2>

- 一維高斯混合模型

![409](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/409.png)

![410](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/410.png)

- 透過GMM，可以順利將這樣一維混合高斯模型順利拆分出來。

![411](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/411.png)

<h2 id="4">二維高斯分佈</h2>

- 同心圓由內到外的三個範圍剛好是68%, 95%, 99%。

![412](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/412.png)

<h2 id="5">二維高斯混合模型</h2>

- 二維高斯混合模型

![413](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/413.png)

- 透過GMM，可以順利將這樣二維混合高斯模型順利拆分出來。

![414](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/414.png)

<h2 id="6">期望最大值算法</h2>

**Expectation - Maximization Algorithm**: 

- 流程:
	- 先初始化K個高斯分佈 (ex. K=2)
	- 對數據進行軟聚類成我們初始化的K個高斯分佈(Expectation)
	- 基於軟聚類重新估計高斯(Maximization)
	- 利用評估對數檢查收斂狀況
		- 收斂: 返回結果
		- 未收斂: 重新step2

![415](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/415.png)

### Step1: 初始化K個高斯分佈

- 先決定只有Ｋ個高斯(這裡的K=2)。
- 在替每個高斯決定隨機均值跟隨機方差。

![416](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/416.png)

### Step2: 對數據進行軟聚類

- 計算每個點對於每個聚類的隸屬度。
- 公式: 正態分布的機率密度函數。
- Z: 隱藏變量(潛在變量)
- 0.99976 = 99.97% 表示我們有99.97%確定這個點屬於cluster A。

![417](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/417.png)

### Step3: 重新估計高斯

- 根據step2的隸屬度重新計算每個高斯的均值跟方差。

![418](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/418.png)

![419](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/419.png)

### Step4: 利用評估對數檢查收斂狀況

![420](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/420.png)

<h2 id="7">期望最大值例子</h2>

- GMM針對不同的初始化，會有不同結果。
	- Initialization, convariance_type

- **在期望最大化算法的第一步中如何初始化高斯参数很重要，好的初始化會幫助算法收敛到最佳值**

![421](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/421.png)

![422](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/422.png)

<h2 id="8">GMM實現</h2>

![423](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/423.png)

<h2 id="9">GMM優缺點</h2>

- 優點:
	- 提供軟聚類(sample有一對多的可能性, ex.文檔分類，每個文檔可能有多個主題)
	- 彈性聚類外觀(一個聚類可能包含另外一個聚類)

- 缺點:
	- 對初始值敏感。
	- 可能收斂到局部最佳。
	- 收斂速度慢。

![424](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/424.png)

<h2 id="10">聚類分析過程</h2>

- **Cluster Analysis**: 如何從數據中得到知識。
	- step1: 特徵選擇和特徵提取。
		- 特徵選擇: 從一組候選特徵中選擇特徵。
		- 特徵提取: 是對特徵進行轉換，以生成新的有用特徵(PCA)。
	- step2: 選擇聚類算法。
		- 根據要做什麼和數據外觀，必須透過實驗來選擇最好的聚類算法。
	- step3: 聚類評價。
		- 透過一些指標來評估聚類的效果如何。
	- step4: 聚類結果解釋。
		- 需要專業領域的知識為結果進行標籤。	

![425](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/425.png)

<h2 id="11">聚類驗證</h2>

- **Cluster Validation**
	- 外部指標: 當數據是標籤的時候使用。
	- 內部指標: 當數據是沒有標籤，大部分的非監督學習都是這情況。
	- 相對指標: 表明兩個聚類哪一個在某個意義上比較好。(基本上所有外部指標都可以當作相對指標)

- 評價指標都是通過**緊湊性和可分性**來定義的。

![426](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/426.png)

<h2 id="12">外部評價指標</h2>

- 常見的外部評價指標

![427](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/427.png)

- 蘭德係數計算

![428](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/428.png)

- 不同cluster結果跟ground truth的蘭德係數。

![429](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/429.png)

<h2 id="13">內部評價指標</h2>

- 常見的內部評價指標

![430](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/430.png)

- 輪廓係數的計算
	- a: 同一個cluster中到其他樣本的平均距離。
	- b: 與距離最近的cluster中的樣本的平均距離。

![431](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/431.png)

- 不同cluster結果的輪廓係數。

![432](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/432.png)

![433](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/433.png)

- 輪廓係數的限制
	- 無法對於雙環結構進行正確評分。
	- 無法對於DBSCAN進行正確評分。
		- [基于密度的聚类验证](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/MLND+documents/10.1.1.707.9034.pdf)

![434](https://github.com/htaiwan/note-Udacity-machine-learning/blob/master/Assets/434.png)